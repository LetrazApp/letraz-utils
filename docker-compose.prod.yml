version: '3.8'

services:
  letraz-scrapper:
    image: letraz-scrapper:latest
    container_name: letraz-scrapper
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      # Server Configuration
      - PORT=8080
      - HOST=0.0.0.0
      - LOG_LEVEL=warn
      - LOG_FORMAT=json
      - LOG_OUTPUT=stdout
      
      # LLM Configuration (Required)
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_PROVIDER=claude
      - LLM_MODEL=claude-3-haiku-20240307
      - LLM_MAX_TOKENS=4096
      - LLM_TEMPERATURE=0.1
      - LLM_TIMEOUT=30s
      
      # Scraper Configuration
      - CAPTCHA_API_KEY=${CAPTCHA_API_KEY}
      - FIRECRAWL_API_KEY=${FIRECRAWL_API_KEY}
      - FIRECRAWL_API_URL=https://api.firecrawl.dev
      - FIRECRAWL_VERSION=v1
      - FIRECRAWL_TIMEOUT=60s
      - FIRECRAWL_MAX_RETRIES=3
      
      # Worker Pool Configuration (Production)
      - WORKER_POOL_SIZE=20
      - WORKER_QUEUE_SIZE=200
      - WORKER_RATE_LIMIT=120
      - WORKER_TIMEOUT=30s
      - WORKER_MAX_RETRIES=3
      
      # Scraper Engine Configuration
      - SCRAPER_TIMEOUT=30s
      - SCRAPER_MAX_RETRIES=3
      - SCRAPER_HEADLESS_MODE=true
      - SCRAPER_STEALTH_MODE=true
      
      # Browser Configuration
      - CHROME_BIN=/usr/bin/chromium-browser
      - CHROME_PATH=/usr/bin/chromium-browser
    
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2.0"
        reservations:
          memory: 1G
          cpus: "1.0"
    
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
        
    # Optional: Mount volumes for persistent data
    # volumes:
    #   - ./logs:/app/logs
    #   - ./tmp:/app/tmp

networks:
  default:
    name: letraz-scrapper-network 